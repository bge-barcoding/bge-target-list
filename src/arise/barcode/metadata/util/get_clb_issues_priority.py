import argparse
import pandas as pd
from datetime import datetime
from collections import Counter
import urllib.request
import shutil
import tempfile


def write_issues_count(title, df, issues_dict, issues_total_count):
    count_issues = []
    for idx, row in df.iterrows():
        if row.taxon_id in issues_dict:
            count_issues += issues_dict[row.taxon_id]

    with open("issues_priority.tsv", "a+") as fw:
        fw.write(f"{title}\n")
        for issue, count in Counter(count_issues).most_common():
            perc = count * 100 / issues_total_count[issue]
            fw.write(f"{issue}\t{count}\t{perc:.2f}\n")


def compare_issues_with_backbone(issues_dict, species_status_file):
    df = pd.read_csv(species_status_file, delimiter="\t")

    # get count of all issues
    issues = []
    for list_issues in issues_dict.values():
        issues += list_issues
    issues_total_count = dict(Counter(issues).most_common())

    with open("issues_priority.tsv", "w") as fw:
        fw.write("Issue name\tCount\tFraction of total species having the same issue\n")

    # inserted case
    write_issues_count("Issues for species added to target list:",
                       df[df["status"] == "ADDED"],
                       issues_dict, issues_total_count)
    write_issues_count("Issues for species ignored because already inserted (ignoring the intra specific epithet):",
                       df[df["reason"] == "ALREADY INSERTED"],
                       issues_dict, issues_total_count)
    write_issues_count("Issues for species added to target list as synonym:",
                       df[df["reason"] == "AS SYNONYM"],
                       issues_dict, issues_total_count)


def get_issues_list(dataset_key):
    new_file, filename = tempfile.mkstemp()
    with urllib.request.urlopen(f"http://api.checklistbank.org/dataset/{dataset_key}/issues") as response, open(new_file, 'wb') as fw:
        shutil.copyfileobj(response, fw)
    with open(filename, 'rb') as fh:
        issues_dict = {}
        for i, line in enumerate(fh.read().decode().split("\n")):
            if i != 0 and line.strip():
                id, issues = line.split(",")
                issues_dict[id] = []
                for issue in issues.split(";"):
                    issues_dict[id].append(issue.strip())
    return issues_dict


if __name__ == '__main__':
    # process command line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument('species_status_file',
                        help="file named 'species_names_added_status.tsv' generated by arise_load_nsr_backbone.py")
    args = parser.parse_args()

    issues_dict = get_issues_list(2014)  # 2014 = NSR
    compare_issues_with_backbone(issues_dict, args.species_status_file)
